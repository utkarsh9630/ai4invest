{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTgsGmyaPnEY",
        "outputId": "6b83aead-be9b-4067-f09a-34feec71b403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Top 5 picks per bucket written to 'top_n_per_category.csv'\n",
            "           vol30     mom30    beta60  future_return risk_label2  pred_return  \\\n",
            "ticker                                                                         \n",
            "PLTR    0.056246  0.422131  1.953529       0.674707        High     0.445761   \n",
            "UBER    0.034605  0.133557  1.158531       0.399767        High     0.269231   \n",
            "APH     0.036773  0.215491  1.188014       0.152121        High     0.146543   \n",
            "BA      0.043870  0.073078  1.350809       0.047560        High    -0.001032   \n",
            "LRCX    0.051898 -0.039072  1.751222       0.046482        High    -0.016942   \n",
            "CVS     0.022361 -0.017191  0.400974       0.540886         Low     0.371514   \n",
            "NEM     0.032055  0.078936  0.609332       0.391950         Low     0.273793   \n",
            "EXC     0.016616  0.042386  0.030017       0.249798         Low     0.215216   \n",
            "T       0.018643  0.030608  0.253283       0.223305         Low     0.171775   \n",
            "MDLZ    0.016877  0.059421  0.086500       0.143074         Low     0.163150   \n",
            "INTC    0.052082 -0.139399  1.418224       0.081847      Medium     0.025270   \n",
            "DVN     0.052577 -0.129809  1.553899       0.030144      Medium     0.008429   \n",
            "SMCI    0.062807 -0.137852  1.852583       0.079065      Medium    -0.040730   \n",
            "CVX     0.029219 -0.159434  0.806128      -0.018774      Medium    -0.065520   \n",
            "SBUX    0.037750 -0.146958  1.105647      -0.045854      Medium    -0.068492   \n",
            "\n",
            "        bucket  \n",
            "ticker          \n",
            "PLTR      High  \n",
            "UBER      High  \n",
            "APH       High  \n",
            "BA        High  \n",
            "LRCX      High  \n",
            "CVS        Low  \n",
            "NEM        Low  \n",
            "EXC        Low  \n",
            "T          Low  \n",
            "MDLZ       Low  \n",
            "INTC    Medium  \n",
            "DVN     Medium  \n",
            "SMCI    Medium  \n",
            "CVX     Medium  \n",
            "SBUX    Medium  \n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3 # needs the input file from stock_classifier_kmeans, should figure out AWS integration later\n",
        "\"\"\"\n",
        "full_pipeline.py\n",
        "\n",
        "1) Read your CSV of tickers & risk buckets.\n",
        "2) Fetch 180 days of EOD closes for each ticker + SPY via Stooq.\n",
        "3) Compute vol30, mom30, beta60, and 90‑day forward return.\n",
        "4) Train a RandomForestRegressor on those features → future_return.\n",
        "5) Predict returns and pick top N in each risk bucket.\n",
        "6) Write results to CSV.\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas_datareader import data as pdr\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# PARAMETERS\n",
        "\n",
        "INPUT_FILE     = \"/content/stock_risk_kmeans_robust.csv\"         # CSV output from stock_classifier_kmeans.ipynb\n",
        "LOOKAHEAD_DAYS = 90                       # days ahead for target return\n",
        "TOP_N          = 5                        # picks per bucket\n",
        "SLEEP_SEC      = 1                        # pause between fetches\n",
        "OUTPUT_FILE    = \"top_n_per_category.csv\" # final recommendations\n",
        "\n",
        "# Load tickers and risk labels\n",
        "df0     = pd.read_csv(INPUT_FILE, index_col=\"ticker\")\n",
        "tickers = df0.index.tolist()\n",
        "\n",
        "#  Helper: fetch Close series from Stooq\n",
        "def fetch_close(symbol):\n",
        "    try:\n",
        "        df = pdr.DataReader(symbol, \"stooq\")\n",
        "        return df.sort_index()[\"Close\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Fetch failed for {symbol}: {e}\")\n",
        "        return pd.Series(dtype=float)\n",
        "\n",
        "\n",
        "# 3) Fetch SPY for beta\n",
        "spy_series = fetch_close(\"SPY\").pct_change().dropna().rename(\"SPY\")\n",
        "time.sleep(SLEEP_SEC)\n",
        "\n",
        "# 4) Fetch each ticker’s history\n",
        "price_hist = {}\n",
        "for sym in tickers:\n",
        "    series = fetch_close(sym)\n",
        "    if not series.empty:\n",
        "        price_hist[sym] = series\n",
        "    else:\n",
        "        print(f\"✗ No data for {sym}, skipping\")\n",
        "    time.sleep(SLEEP_SEC)\n",
        "\n",
        "# 5) Build feature + target rows\n",
        "rows = []\n",
        "for sym, series in price_hist.items():\n",
        "    if len(series) < LOOKAHEAD_DAYS + 1:\n",
        "        print(f\"{sym}: insufficient history, skipping\")\n",
        "        continue\n",
        "\n",
        "    # a) 30‑day volatility\n",
        "    ret = series.pct_change().dropna()\n",
        "    vol30 = ret.rolling(30).std().iloc[-1] if len(ret) >= 30 else np.nan\n",
        "\n",
        "    # b) 30‑day momentum\n",
        "    mom30 = series.pct_change(30).iloc[-1] if len(series) >= 30 else np.nan\n",
        "\n",
        "    # c) 60‑day beta vs SPY\n",
        "    comb = pd.concat([ret, spy_series], axis=1, join=\"inner\").dropna()\n",
        "    if len(comb) < 60:\n",
        "        beta60 = np.nan\n",
        "    else:\n",
        "        cov = comb.iloc[:,0].rolling(60).cov(comb[\"SPY\"])\n",
        "        var = comb[\"SPY\"].rolling(60).var()\n",
        "        beta60 = (cov/var).iloc[-1]\n",
        "\n",
        "    # d) 90‑day forward return\n",
        "    future_ret = series.iloc[-1] / series.shift(LOOKAHEAD_DAYS).iloc[-1] - 1\n",
        "\n",
        "    # e) risk label\n",
        "    risk = df0.loc[sym, \"risk_label2\"]\n",
        "\n",
        "    rows.append({\n",
        "        \"ticker\":         sym,\n",
        "        \"vol30\":          vol30,\n",
        "        \"mom30\":          mom30,\n",
        "        \"beta60\":         beta60,\n",
        "        \"future_return\":  future_ret,\n",
        "        \"risk_label2\":    risk\n",
        "    })\n",
        "\n",
        "feat_df = pd.DataFrame(rows).set_index(\"ticker\")\n",
        "if feat_df.empty:\n",
        "    raise RuntimeError(\"No valid data to build features/targets!\")\n",
        "\n",
        "#train the model\n",
        "X_raw = feat_df[[\"vol30\",\"mom30\",\"beta60\"]]\n",
        "y     = feat_df[\"future_return\"]\n",
        "\n",
        "# Impute missing feature values\n",
        "imp    = SimpleImputer(strategy=\"median\")\n",
        "X_imp  = imp.fit_transform(X_raw)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imp)\n",
        "\n",
        "\n",
        "# 7) Train RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_scaled, y)\n",
        "\n",
        "feat_df[\"pred_return\"] = model.predict(X_scaled)\n",
        "\n",
        "# 9) Select top N in each risk bucket\n",
        "\n",
        "results = []\n",
        "for bucket in feat_df[\"risk_label2\"].unique():\n",
        "    subset = feat_df[feat_df[\"risk_label2\"] == bucket]\n",
        "    topn   = subset.nlargest(TOP_N, \"pred_return\").copy()\n",
        "    topn[\"bucket\"] = bucket\n",
        "    results.append(topn)\n",
        "\n",
        "final_df = pd.concat(results)\n",
        "final_df.to_csv(OUTPUT_FILE)\n",
        "\n",
        "print(f\"Top {TOP_N} picks per bucket written to '{OUTPUT_FILE}'\")\n",
        "print(final_df)\n"
      ]
    }
  ]
}